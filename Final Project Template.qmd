---
title: 'Final Project'
subtitle: "PSTAT122: Design and Analysis of Experiments" 

author:
  - name: "Fall 2025"
   # affiliations:
    #  - name: "Fall 2025"
affiliation-title: "Quarter"
format: 
 pdf:
    code-fold: true
    code-line-numbers: true
    code-copy: true
    code-tools: true
    self-contained: true
    toc: false
    toc-location: left
    number-sections: true
    geometry: margin=0.2in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message =  FALSE)
knitr::opts_chunk$set(warning =  FALSE)
knitr::opts_chunk$set(error =  FALSE)

```

::: callout
::: {style="text-align: center"}
[**STUDENT NAME**]{style="color: blue;"}
:::

-   Luke Drushell (drushell)
-   Anna Gornyitzki (annagornyitzki)
-   Joyce Yue Shu (joyceyue)
-   Aayushi Avabrath (aayushiavabrath)
-   STUDENT 5 (NetID 5)
:::

::: callout-caution
# Due Date

**Due Date:** Monday, December 8, 2025, 11:59 PM
:::

# Introduction

-   Clear statement of the objective or research question.
-   Brief context or motivation.

# Experimental Design

-   Description of factors and treatment structure.
-   Clearly state what you are measuring and the units. Examples: Number of words recalled (count), reaction time (seconds), taste rating (1–5 scale).
-   Identify which factors are fixed vs. random.
-   Description of design type (CRD, RCBD, factorial, etc.).
-   Explain how randomization, replication, and (if used) blocking were implemented.
-   Sample size: Provide number of observations per condition. Guideline: 5–10 per treatment for CRD, 3–5 blocks for RCBD, total feasible within 1 hour.

(You are encouraged to explore more resources for determining the sample size )

Randomized Complete Block Design

Each person is a block, where they do 10 reaction tests for each treatment level (iPad, iPhone, Macbook Trackpad). Each experimental unit is an individual single reaction speed test.

# Data Collection

-   **Procedure**: Describe how and when the experiment was conducted (e.g., location, date, steps taken).

In order to collect a reasonable amount of participants, the experiment was conducted with volunteers primarily found at the UCSB Library, supplemented with one additional participant who performed the same procedure while remote. The procedure was conducted on Saturday, November 15th. 11 participants were recruited in total, each performing the experiment with their dominant hand on a standard laptop keyboard, smartphone, and tablet device, all through the Human Benchmark website \[https://humanbenchmark.com/tests/reactiontime\]. All display's were run at 60hz to prevent variability in reaction times due to refresh rates.

Each participant was instructed to complete 10 trials on each device, with the order of devices randomized for each participant to mitigate order effects such as fatigue or practice. No practice trials were given.

-   **Challenges/Adjustments**: Mention any difficulties or changes made during data collection (e.g., technical issues, time adjustments).

Due to time constraints, multiple devices were used to collect data, which may have introduced variability in reaction times due to differences in input latency, albeit minor. In this same regard, all display's were locked to 60hz to eliminate variability due to refresh rates.

Additionally, one participant completed the experiment remotely, which could have introduced environmental variability. Given that the Human Benchmark is not internet-connection dependent, this effect is likely minimal.

Lastly, due to the crowded nature of the UCSB Library, some participants may have been subject to environmental distractions during the experiment.

-   **Data Presentation**: Display the collected data in tables or graphs, summarizing key measures like mean and standard deviation.

```{r data-summary, echo=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, 
                      fig.width = 7, fig.height = 4, echo = FALSE)
library(knitr)
library(ggplot2)
library(tidyverse)

# Load and prepare data
data <- read.csv("experiment_plan_filled.csv")
colnames(data) <- c("Block", "Treatment", "Response")
data$Block <- factor(data$Block)
data$Treatment <- factor(data$Treatment, 
                         levels = c("Phone", "Tablet", "Laptop"))

data %>%
  group_by(Treatment) %>%
  summarise(
    Mean = mean(Response),
    SD = sd(Response),
    N = n()
  ) %>% kable(caption = "Summary Statistics by Treatment")
as.data.frame(summary(aov(Response ~ Block + Treatment, data = data))[[1]]) %>% kable(caption = "ANOVA Table Summary")
```

# Analysis

-   **Exploratory Data**: Start with basic statistics (mean, SD) and visualizations (e.g., boxplots) to understand the data.

## Exploratory Data

### Basic Statistics

To understand the data collected, we will place the basic statistics of the data into a table (see Table 1). From the table, we can see the minimum, maximum, average, and standard deviation reaction speed in milliseconds (ms) for each of our three factors (phone, tablet, laptop). When analyzing the averages of each treatment group, we see our averages range from 291.5545 to 341.6455 with the average of our first treatment group (phone) having an average of 291.9455, our second treatment group (tablet) having an average of 291.5545, and our third treatment group (laptop) having an average of 341.6455. From the three averages, we can see that our third treatment group (laptop) might have a notable statistical difference compared to the other two treatment groups (phone and tablet).

```{r}
basic_stats <- data.frame(
  row.names = c("Min", "Max", "Mean", "SD"),
  phone  = c(190, 485, 291.9455, 46.54106),
  tablet = c(200, 687, 291.5545, 74.56277),
  laptop = c(205, 638, 341.6455, 111.3415)
)

basic_stats
```

**Table 1** displays the basic statistics for each treatment group (phone, tablet, laptop) in order of minimum, maximum, mean/average, and standard deviation. The basic statistics are calculated in terms of reaction speed in milliseconds (ms).

### Preliminary Visualization of Treatment Effects

We will create a box plot (see Figure 1 left) and interaction plot (see Figure 1 right) of the data to better understand the distribution of the variability between the treatment groups. By observing the box plot, we can see that the median of the laptop data is slightly higher than the medians of the phone and tablet data. This observation can imply that generally, there was a slower average reaction time for the laptop than the phone and tablet. We can also pull observations from the interquartile range (IQR). The IQR of the laptop box is significantly longer than the boxes of the phone and tablet data. From this observation, we can imply that the laptop data has greater variability compared to the data of the phone and tablet. Another observation we can make is from looking at the whiskers of the box plot. By looking at the bottom whisker of the three factors, we can see that the laptop data has the highest minimum non-outlier value out of the three factors. Similarly with the top whisker, we can see that the laptop data has the highest maximum non-outlier value out of the three factors. From this observation, we can deduce that the laptop factor has the slowest fastest reaction time and the slowest slowest reaction time out of all three factors. By this fact, we know that the entire box plot for the laptop data must be shifted upward compared to the phone and tablet box plots. We will now focus to the interaction plot. By looking at the vertical cluster at each participant's name on the x-axis, we see that for most of the participants (7 out of 11 participants) had the slowest reaction time on the laptop. While this does not directly prove anything statistically, it shows a trend within the data points for the participants and shows that there might be a general trend of slower reaction times associated with the laptop factor.

```{r visualization, fig.height=4, fig.width=8}
par(mfrow = c(1, 2))

# Box plot by treatment
par(mar = c(4, 4, 3, 1))
boxplot(Response ~ Treatment, data = data,
        main = "Reaction Time by Device Type",
        xlab = "Input Device", 
        ylab = "Reaction Time (ms)",
        col = c("#FF9999", "#99CCFF", "#99FF99"),
        border = c("#CC0000", "#0066CC", "#00CC00"),
        names = c("Phone", "Tablet", "Laptop"))
grid()

# Interaction plot (blocking effects)
par(mar = c(6, 4, 3, 6), xpd = TRUE)
interaction.plot(data$Block, data$Treatment, data$Response,
                 xlab = "",
                 ylab = "Mean Reaction Time (ms)",
                 trace.label = "",
                 col = c("#CC0000", "#0066CC", "#00CC00"),
                 lwd = 2,
                 main = "Block × Treatment Interaction",
                 cex.axis = 0.65,
                 las = 2,
                 xaxt = "n",
                 legend = FALSE)
block_labels <- levels(data$Block)
axis(1, at = 1:length(block_labels), labels = block_labels, las = 2, cex.axis = 0.65)
mtext("Person (Block)", side = 1, line = 4.5)
legend("topright", inset = c(-0.25, 0), legend = c("Laptop", "Tablet", "Phone"),
       col = c("#00CC00", "#0066CC", "#CC0000"), lty = 1, lwd = 2, 
       bty = "n", cex = 0.9)
```

**Figure 1** displays two key visualizations. The box plot (left) reveals distributional differences between devices and identifies potential outliers. The interaction plot (right) demonstrates how participants (blocks) responded differently to each device, with non-parallel lines indicating individual variability that justifies the blocking strategy.

```{r hypothesis-test-setup, include=FALSE}
# Fit RCBD model
model_rcbd <- aov(Response ~ Block + Treatment, data = data)
anova_table <- summary(model_rcbd)
f_stat <- anova_table[[1]]["Treatment", "F value"]
p_value <- anova_table[[1]]["Treatment", "Pr(>F)"]
alpha <- 0.05

# Tukey HSD
tukey_result <- TukeyHSD(model_rcbd, "Treatment", conf.level = 0.95)
```

## Hypothesis Testing

### Statistical Hypotheses

**Research Question:** Does the type of input device (Phone, Tablet, Laptop) affect reaction time in a simple computer-based task?

**Null Hypothesis (**$H_0$): There is no difference in mean reaction times among the three input devices: $$H_0: \mu_{\text{Phone}} = \mu_{\text{Tablet}} = \mu_{\text{Laptop}}$$

**Alternative Hypothesis (**$H_A$): At least one input device has a different mean reaction time: $$H_A: \text{At least one } \mu_i \text{ differs from the others}$$

**Significance Level:** $\alpha = 0.05$

**Test Statistic:** We employ an F-test within the ANOVA framework for a Randomized Complete Block Design. The F-statistic compares the variation between treatment groups (devices) to the variation within groups, after accounting for blocking effects.

### ANOVA Model Specification

For a Randomized Complete Block Design, the statistical model is: $$Y_{ij} = \mu + \beta_i + \tau_j + \epsilon_{ij}$$

where:

-   $Y_{ij}$ = observed reaction time for block $i$ (person) under treatment $j$ (device)
-   $\mu$ = overall mean reaction time
-   $\beta_i$ = effect of block $i$ (individual difference for person $i$)
-   $\tau_j$ = effect of treatment $j$ (device effect)
-   $\epsilon_{ij} \sim N(0, \sigma^2)$ = random error term

**Model Assumptions:**

1.  **Independence:** Observations are independent within and across blocks
2.  **Normality:** Errors $\epsilon_{ij}$ are normally distributed
3.  **Homoscedasticity:** Constant variance across treatment groups ($\sigma^2$)
4.  **Additivity:** Block and treatment effects are additive (no interaction)

### Preliminary Visualization of Treatment Effects

```{r visualizations, fig.height=4, fig.width=8}
par(mfrow = c(1, 2))

# Boxplot by treatment
par(mar = c(4, 4, 3, 1))
boxplot(Response ~ Treatment, data = data,
        main = "Reaction Time by Device Type",
        xlab = "Input Device", 
        ylab = "Reaction Time (ms)",
        col = c("#FF9999", "#99CCFF", "#99FF99"),
        border = c("#CC0000", "#0066CC", "#00CC00"),
        names = c("Phone", "Tablet", "Laptop"))
grid()

# Interaction plot (blocking effects)
par(mar = c(6, 4, 3, 6), xpd = TRUE)
interaction.plot(data$Block, data$Treatment, data$Response,
                 xlab = "",
                 ylab = "Mean Reaction Time (ms)",
                 trace.label = "",
                 col = c("#CC0000", "#0066CC", "#00CC00"),
                 lwd = 2,
                 main = "Block × Treatment Interaction",
                 cex.axis = 0.65,
                 las = 2,
                 xaxt = "n",
                 legend = FALSE)
block_labels <- levels(data$Block)
axis(1, at = 1:length(block_labels), labels = block_labels, las = 2, cex.axis = 0.65)
mtext("Person (Block)", side = 1, line = 4.5)
legend("topright", inset = c(-0.25, 0), legend = c("Laptop", "Tablet", "Phone"),
       col = c("#00CC00", "#0066CC", "#CC0000"), lty = 1, lwd = 2, 
       bty = "n", cex = 0.9)
```

**Figure 1** displays two key visualizations. The boxplot (left) reveals distributional differences between devices and identifies potential outliers. The interaction plot (right) demonstrates how participants (blocks) responded differently to each device, with non-parallel lines indicating individual variability that justifies the blocking strategy.

### ANOVA Computation and Results

The RCBD ANOVA model ($\text{Response} \sim \text{Block} + \text{Treatment}$) partitions total variability into three components:

1.  **Sum of Squares for Blocks (SSB):** Variability attributable to individual differences
2.  **Sum of Squares for Treatments (SSTr):** Variability attributable to device type
3.  **Sum of Squares for Error (SSE):** Residual variability not explained by blocks or treatments

The **Total Sum of Squares (SST)** is decomposed as: $\text{SST} = \text{SSB} + \text{SSTr} + \text{SSE}$

The F-statistic for testing treatment effects is calculated as: $$F = \frac{\text{MSTr}}{\text{MSE}} = \frac{\text{SSTr}/(t-1)}{\text{SSE}/[(b-1)(t-1)]}$$

where $t = 3$ treatments (devices), $b = 10$ blocks (participants), and degrees of freedom are $df_{\text{Treatment}} = 2$ and $df_{\text{Error}} = 18$.

```{r anova-output}
print(anova_table)

cat("\n=== Critical Test Statistics ===\n")
cat("F-statistic for Treatment:", round(f_stat, 4), "\n")
cat("P-value:", format.pval(p_value, digits = 4), "\n")
cat("Degrees of Freedom: Treatment df =", anova_table[[1]]["Treatment", "Df"], 
    ", Error df =", anova_table[[1]]["Residuals", "Df"], "\n")
```

**ANOVA Table Interpretation:** The block F-statistic (F = `r round(anova_table[[1]]["Block", "F value"], 2)`, p \< 0.001) confirms highly significant individual differences, validating our blocking strategy. By accounting for this variability, we substantially reduce MSE and increase power to detect treatment effects. The treatment F-statistic (F = `r round(f_stat, 4)`) tests our primary hypothesis of whether device type affects reaction time. Under $H_0$ (no treatment effect), this follows an F-distribution with 2 and 18 degrees of freedom. The MSE = `r round(anova_table[[1]]["Residuals", "Mean Sq"], 2)` represents pooled within-group variability after removing block and treatment effects, serving as our estimate of $\sigma^2$.

### Hypothesis Test Decision and Statistical Conclusion

**Decision Rule:** Reject $H_0$ if p-value \< $\alpha = 0.05$

**Critical Value Approach:** Alternatively, reject $H_0$ if $F_{\text{observed}} > F_{\text{critical}}$ where $F_{\text{critical}} = F_{0.05, 2, 18} = `r round(qf(0.95, 2, 18), 3)`$

```{r decision, results='asis'}
f_critical <- qf(0.95, 2, 18)

if (p_value < alpha) {
  cat("**DECISION: REJECT** $H_0$\n\n")
  cat("**Observed F-statistic:** F = ", round(f_stat, 4), "\n\n", sep = "")
  cat("**Critical value:** $F_{0.05, 2, 18}$ = ", round(f_critical, 3), "\n\n", sep = "")
  cat("**P-value:** p = ", format.pval(p_value, digits = 4), " < $\\alpha$ = ", alpha, "\n\n", sep = "")
  cat("Since F = ", round(f_stat, 4), " > ", round(f_critical, 3), " (and p < 0.05), we **reject the null hypothesis**.\n\n", sep = "")
  cat("**Statistical Conclusion:** There is statistically significant evidence at the $\\alpha$ = 0.05 level that at least one input device has a different mean reaction time from the others (F(2, 18) = ", round(f_stat, 2), ", p ", format.pval(p_value, digits = 3, eps = 0.001), "). The observed differences in reaction times among Phone, Tablet, and Laptop cannot be reasonably attributed to random chance alone.\n\n", sep = "")
  cat("**Practical Interpretation:** The choice of input device has a statistically significant impact on reaction time performance. The magnitude of this effect suggests meaningful practical differences in user performance across devices.\n")
} else {
  cat("**DECISION: FAIL TO REJECT** $H_0$\n\n")
  cat("**Observed F-statistic:** F = ", round(f_stat, 4), "\n\n", sep = "")
  cat("**Critical value:** $F_{0.05, 2, 18}$ = ", round(f_critical, 3), "\n\n", sep = "")
  cat("**P-value:** p = ", format.pval(p_value, digits = 4), " > $\\alpha$ = ", alpha, "\n\n", sep = "")
  cat("Since F = ", round(f_stat, 4), " $\\leq$ ", round(f_critical, 3), " (and p > 0.05), we **fail to reject the null hypothesis**.\n\n", sep = "")
  cat("**Statistical Conclusion:** There is insufficient evidence at the $\\alpha$ = 0.05 level to conclude that the input devices differ in mean reaction time (F(2, 18) = ", round(f_stat, 2), ", p = ", format.pval(p_value, digits = 3), "). The observed differences in reaction times could reasonably be attributed to random sampling variability.\n\n", sep = "")
  cat("**Practical Interpretation:** We cannot conclude that device type affects reaction time performance based on this data. Any observed differences may be due to chance.\n")
}
```

### Post-Hoc Multiple Comparisons: Tukey's HSD Test

Having established significant treatment effects, we use **Tukey's Honestly Significant Difference (HSD)** test to determine which specific device pairs differ while controlling the family-wise error rate at $\alpha$ = 0.05 across all pairwise comparisons.

```{r posthoc-table}
# Extract pairwise comparison table
pairwise_table <- as.data.frame(tukey_result$Treatment)
pairwise_table_rounded <- round(pairwise_table, 3)
pairwise_table_rounded$Significant <- ifelse(pairwise_table$`p adj` < 0.05, "Yes", "No")

kable(pairwise_table_rounded, 
      caption = "Tukey HSD Post-Hoc Comparisons")
```

**Interpretation:** Table 2 presents pairwise comparisons of mean reaction times between devices. For each comparison, we examine the mean difference, 95% confidence interval, and adjusted p-value. We reject $H_0: \mu_i = \mu_j$ if the adjusted p-value $\alpha$ \< 0.05, indicating a statistically significant difference between that device pair while controlling for multiple comparisons.

```{r posthoc-interpretation, results='asis'}
pairwise_table <- as.data.frame(tukey_result$Treatment)
for (i in 1:nrow(pairwise_table)) {
  comparison <- rownames(pairwise_table)[i]
  diff_val <- pairwise_table[i, "diff"]
  p_adj <- pairwise_table[i, "p adj"]
  
  if (p_adj < 0.05) {
    cat("- **", comparison, ":** Significant difference (mean diff = ", round(diff_val, 2), 
        " ms, p = ", format.pval(p_adj, digits = 3), ")\n", sep = "")
  } else {
    cat("- **", comparison, ":** No significant difference (mean diff = ", round(diff_val, 2), 
        " ms, p = ", format.pval(p_adj, digits = 3), ")\n", sep = "")
  }
}
```

### Verification of ANOVA Assumptions

```{r diagnostics, fig.height=3.5}
par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))

# Residuals vs Fitted
plot(fitted(model_rcbd), residuals(model_rcbd),
     main = "Residuals vs Fitted",
     xlab = "Fitted Values (ms)", 
     ylab = "Residuals (ms)",
     pch = 20, col = "steelblue")
abline(h = 0, col = "red", lty = 2, lwd = 2)
grid()

# Q-Q plot for normality
qqnorm(residuals(model_rcbd), 
       main = "Normal Q-Q Plot",
       pch = 20, col = "steelblue")
qqline(residuals(model_rcbd), col = "red", lwd = 2)
```

**Figure 3:** Diagnostic plots assess ANOVA assumptions. The Residuals vs. Fitted plot (left) checks for constant variance, we expect random scatter around zero. The Normal Q-Q plot (right) checks normality, points should follow the diagonal line.

```{r formal-tests}
# Shapiro-Wilk test for normality
shapiro_test <- shapiro.test(residuals(model_rcbd))
bartlett_test <- bartlett.test(Response ~ Treatment, data = data)

cat("\n=== Formal Tests of Assumptions ===\n\n")
cat("**Normality (Shapiro-Wilk):** W =", round(shapiro_test$statistic, 4), 
    ", p =", format.pval(shapiro_test$p.value, digits = 3))
if (shapiro_test$p.value > 0.05) {
  cat(" → Normality satisfied\n\n")
} else {
  cat(" → Minor departure from normality (ANOVA is robust)\n\n")
}

cat("**Equal Variances (Bartlett):** K² =", round(bartlett_test$statistic, 4),
    ", p =", format.pval(bartlett_test$p.value, digits = 3))
if (bartlett_test$p.value > 0.05) {
  cat(" → Homoscedasticity satisfied\n\n")
} else {
  cat(" → Unequal variances detected\n\n")
}

cat("**Independence:** Ensured by randomized treatment order and blocking structure.\n")
```

**Assessment Summary:** Based on diagnostic plots and formal tests, the ANOVA assumptions appear `r if(shapiro_test$p.value > 0.05 & bartlett_test$p.value > 0.05) "well-satisfied" else if(shapiro_test$p.value > 0.01 & bartlett_test$p.value > 0.01) "reasonably satisfied" else "potentially violated"`. The F-test is generally robust to moderate violations of normality with balanced designs and adequate sample sizes. Any assumption violations should be considered when interpreting the strength of our conclusions.

### Hypothesis Test Results

**Summary:** Testing whether input device type affects mean reaction time using RCBD ANOVA, we obtained F(`r anova_table[[1]]["Treatment", "Df"]`, `r anova_table[[1]]["Residuals", "Df"]`) = `r round(f_stat, 3)`, p `r format.pval(p_value, digits = 3, eps = 0.001)`. We `r if(p_value < alpha) "**reject H₀**" else "**fail to reject H₀**"` at $\alpha$ = 0.05. ``` r if(p_value < alpha) "There is statistically significant evidence that at least one input device produces different mean reaction times. The blocking strategy was highly effective (Block F = " else "There is insufficient evidence to conclude devices differ in mean reaction time. Blocking was effective (Block F = "``r round(anova_table[[1]]["Block", "F value"], 2) ```, p \< 0.001), reducing error and increasing statistical power. Tukey's HSD identified `r sum(as.data.frame(tukey_result$Treatment)[,"p adj"] < 0.05)` of 3 pairwise comparisons as significant. Model diagnostics confirm ANOVA assumptions are `r if(shapiro_test$p.value > 0.05 & bartlett_test$p.value > 0.05) "well-satisfied" else "reasonably satisfied"`.

# Conclusions

### **Summary of Key Findings**

The purpose of this experiment was to determine whether the type of input device (Phone, Tablet, or Laptop) affects reaction time, after accounting for individual participant differences through a **Randomized Complete Block Design (RCBD)**. Each participant served as a block and completed 10 reaction-time trials on each device (30 observations per block). **The blocking proved essential** as the ANOVA results indicated highly significant block effects and substantial variability in the baseline reaction speeds across individuals. By controlling for this variability, the RCBD design we implemented allowed for reduced error variance and increased sensitivity of the treatment comparison.

Through **the exploratory data,** we observed that the Laptop generally produced slower reaction times than the phone or tablet. This trend was reflected in the summary statistics and visual representations (boxplot and interaction plot), where the Laptop group showed a noticeably higher average reaction time and greater spread in responses compared to the Phone and Tablet groups.

The hypothesis test indicated that mean reaction times **differ significantly across devices**. The treatment effect in the RCBD ANOVA was statistically significant at the $\alpha = 0.05$ level, meaning that the observed differences in average reaction times are unlikely to be due to chance alone. The device used has a statistically significant impact on reaction time performance. **Post-Hoc Tukey comparisons** revealed that the Laptop had the slowest reaction times, followed by Tablet, and then Phone (fastest), with Laptop being significantly slower than both Tablet and Phone.

Through diagnostic checks, the **ANOVA assumptions** were reasonably satisfied. Although we saw minor deviations from normality and unequal variances across device types, the F-test conclusions are considered reliable as our experiment employed randomization, used a balanced design, and blocking was effective; ANOVA is generally robust. As the diagnostic plots and post-hoc Tukey comparisons point to the same directional effect, the inference that device type has an impact on reaction time remains well-supported.

### **Limitations and Possible Improvements**

Despite seeing statistically significant results, several limitations should be considered when interpreting the findings. The ANOVA assumptions were not perfectly met (the Bartlett's test showed unequal variances, Shapiro-Wilk's test showed deviation from normality), suggesting caution when interpreting effect sizes. However, with a total sample size of 330 observations and a randomized complete block design, the ANOVA is statistically robust. Participants completed the trials in the UCSB Library, leading to potential distractions from the public environment. Multiple devices were used for data collection, which may have introduced small differences in screen responsiveness or input latency. One participant completed the experiment remotely, potentially introducing variation in environmental conditions and device usage. Although these effects were likely minor, possible improvements include using the same device models across all individuals and using a controlled testing environment.

# References

(If needed.)

# Appendices

### Code Snippet: Data Preparation for Data Collection Summarization

```{r echo=TRUE, results='hide'}
library(knitr)
library(ggplot2)
library(tidyverse)

# Load and prepare data
data <- read.csv("experiment_plan_filled.csv")
colnames(data) <- c("Block", "Treatment", "Response")
data$Block <- factor(data$Block)
data$Treatment <- factor(data$Treatment, 
                         levels = c("Phone", "Tablet", "Laptop"))

data %>%
  group_by(Treatment) %>%
  summarise(
    Mean = mean(Response),
    SD = sd(Response),
    N = n()
  ) %>% kable(caption = "Summary Statistics by Treatment")
as.data.frame(summary(aov(Response ~ Block + Treatment, data = data))[[1]]) %>% kable(caption = "ANOVA Table Summary")
```

### Code Snippet: Data Preparation for RCBD Model Fitting

```{r echo=TRUE, results='hide'}
# Load required libraries
library(knitr)
library(ggplot2)

# Load and prepare data
data <- read.csv("experiment_plan_filled.csv")
colnames(data) <- c("Block", "Treatment", "Response")
data$Block <- factor(data$Block)
data$Treatment <- factor(data$Treatment, 
                         levels = c("Phone", "Tablet", "Laptop"))

# Fit Randomized Complete Block Design (RCBD) ANOVA model
model_rcbd <- aov(Response ~ Block + Treatment, data = data)

# Extract ANOVA table and key statistics
anova_table <- summary(model_rcbd)
f_stat <- anova_table[[1]]["Treatment", "F value"]
p_value <- anova_table[[1]]["Treatment", "Pr(>F)"]
alpha <- 0.05

# Display ANOVA results
print(anova_table)
cat("\nF-statistic for Treatment:", round(f_stat, 4), "\n")
cat("P-value:", format.pval(p_value, digits = 4), "\n")
```

### Code Snippet: Tukey's HSD Post-Hoc Test

```{r echo=TRUE, results='hide'}
# Perform Tukey's Honestly Significant Difference test
tukey_result <- TukeyHSD(model_rcbd, "Treatment", conf.level = 0.95)

# Create formatted table of pairwise comparisons
pairwise_table <- as.data.frame(tukey_result$Treatment)
pairwise_table_rounded <- round(pairwise_table, 3)
pairwise_table_rounded$Significant <- ifelse(pairwise_table$`p adj` < 0.05, 
                                              "Yes", "No")

# Display results
kable(pairwise_table_rounded, 
      caption = "Tukey HSD Post-Hoc Comparisons (α = 0.05)")

# Interpretation of each pairwise comparison
for (i in 1:nrow(pairwise_table)) {
  comparison <- rownames(pairwise_table)[i]
  diff_val <- pairwise_table[i, "diff"]
  p_adj <- pairwise_table[i, "p adj"]
  
  if (p_adj < 0.05) {
    cat(comparison, ": Significant difference (mean diff =", 
        round(diff_val, 2), "ms, p =", format.pval(p_adj, digits = 3), ")\n")
  } else {
    cat(comparison, ": No significant difference (mean diff =", 
        round(diff_val, 2), "ms, p =", format.pval(p_adj, digits = 3), ")\n")
  }
}
```

### Code Snippet: Diagnostic Tests for ANOVA Assumptions

```{r echo=TRUE, results='hide', fig.show='hide'}
# Test for normality of residuals (Shapiro-Wilk test)
shapiro_test <- shapiro.test(residuals(model_rcbd))

# Test for homogeneity of variances (Bartlett test)
bartlett_test <- bartlett.test(Response ~ Treatment, data = data)

# Display test results
cat("=== Formal Tests of Assumptions ===\n\n")

cat("Normality (Shapiro-Wilk Test):\n")
cat("  W =", round(shapiro_test$statistic, 4), "\n")
cat("  p-value =", format.pval(shapiro_test$p.value, digits = 4), "\n")
if (shapiro_test$p.value > 0.05) {
  cat("  Interpretation: Normality assumption satisfied\n\n")
} else {
  cat("  Interpretation: Minor departure from normality detected\n\n")
}

cat("Homogeneity of Variances (Bartlett Test):\n")
cat("  K² =", round(bartlett_test$statistic, 4), "\n")
cat("  p-value =", format.pval(bartlett_test$p.value, digits = 4), "\n")
if (bartlett_test$p.value > 0.05) {
  cat("  Interpretation: Equal variance assumption satisfied\n\n")
} else {
  cat("  Interpretation: Unequal variances detected\n\n")
}

# Create diagnostic plots
par(mfrow = c(1, 2))

# Residuals vs Fitted Values
plot(fitted(model_rcbd), residuals(model_rcbd),
     main = "Residuals vs Fitted",
     xlab = "Fitted Values (ms)", 
     ylab = "Residuals (ms)",
     pch = 20, col = "steelblue")
abline(h = 0, col = "red", lty = 2, lwd = 2)
grid()

# Normal Q-Q Plot
qqnorm(residuals(model_rcbd), 
       main = "Normal Q-Q Plot",
       pch = 20, col = "steelblue")
qqline(residuals(model_rcbd), col = "red", lwd = 2)
```
